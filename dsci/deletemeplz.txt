## Overview

We'll perform the following steps:

1. **Load and Prepare the Data:**
   - Load the Iris dataset.
   - Split it into training and testing sets.
   - Convert NumPy arrays to lists for educational purposes.

2. **Implement a Decision Tree Classifier:**
   - Create a simplified `DecisionTree` class from scratch.
   - Include methods for fitting the model and making predictions.

3. **Define Hyperparameter Search Space:**
   - Specify ranges for hyperparameters like `max_depth`, `min_samples_split`, `min_samples_leaf`, and `criterion`.

4. **Random Search for Hyperparameter Tuning:**
   - Randomly sample combinations of hyperparameters.
   - Train and evaluate the model for each combination.
   - Select the best-performing set of hyperparameters.

5. **Error Analysis:**
   - Identify and display misclassified instances.
   - Compute and display the confusion matrix.

Let's dive into each step in detail.

---

## 1. Load and Prepare the Data

We'll start by loading the Iris dataset, splitting it into training and testing sets, and converting the data from NumPy arrays to lists.

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from random import choice
from collections import Counter

# Constants
TEST_SIZE = 0.3
N_ITERATIONS = 100
RANDOM_STATE = 73
np.random.seed(RANDOM_STATE)

def np_to_list(numpy_array):
    """
    Converts numpy.ndarray to lists.
    ------------------------------------
    INPUT:
        numpy_array: (np.ndarray) 

    OUTPUT:
        (list)
    """
    return numpy_array.tolist()

def load_and_prepare_data():
    """
    Loads the Iris dataset, splits it into training and testing sets,
    and converts the data to lists.
    ----------------------------------------------------
    INPUT:
        None

    OUTPUT:
        X_train: (list) Training data features
        X_test: (list) Test data features
        y_train: (list) Training data labels
        y_test: (list) Test data labels
    """
    iris = load_iris()
    X, y = iris.data, iris.target
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )
    # Convert to lists
    X_train = np_to_list(X_train)
    X_test = np_to_list(X_test)
    y_train = np_to_list(y_train)
    y_test = np_to_list(y_test)
    return X_train, X_test, y_train, y_test
```

**Explanation:**

- **Loading Data:** We use `load_iris()` from `sklearn.datasets` to load the Iris dataset.
- **Splitting Data:** `train_test_split` splits the data into training and testing sets based on the specified `TEST_SIZE` and `RANDOM_STATE` for reproducibility.
- **Converting to Lists:** The `np_to_list` function converts NumPy arrays to Python lists, making it easier to work with in custom implementations.

---

## 2. Implement a Decision Tree Classifier

We'll create a simplified `DecisionTree` class that can handle classification tasks. This implementation is basic and intended for educational purposes.

```python
class TreeNode:
    """
    A node in the decision tree.
    """
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):
        """
        Initializes the TreeNode.
        """
        self.feature_index = feature_index      # Index of the feature to split on
        self.threshold = threshold              # Threshold value for the split
        self.left = left                        # Left child
        self.right = right                      # Right child
        self.value = value                      # Class label if it's a leaf node

class DecisionTree:
    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, criterion="gini"):
        """
        Initializes the Decision Tree.
        --------------------------------------------------------
        INPUT:
            max_depth: (int) Maximum depth of the tree
            min_samples_split: (int) Minimum number of samples required to split a node
            min_samples_leaf: (int) Minimum number of samples required at a leaf node
            criterion: (str) "gini" or "entropy" for the split criterion
        """
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        if criterion not in ["gini", "entropy"]:
            raise ValueError("Criterion must be 'gini' or 'entropy'")
        self.criterion = criterion
        self.root = None

    def fit(self, X, y):
        """
        Builds the decision tree.
        --------------------------------------------------------
        INPUT:
            X: (list of lists) Training data features
            y: (list) Training data labels
        OUTPUT:
            None
        """
        self.root = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        """
        Recursively builds the tree.
        """
        num_samples = len(y)
        num_features = len(X[0])
        num_labels = len(set(y))

        # Stopping criteria
        if (self.max_depth is not None and depth >= self.max_depth) or \
           num_labels == 1 or \
           num_samples < self.min_samples_split:
            leaf_value = self._most_common_label(y)
            return TreeNode(value=leaf_value)

        # Find the best split
        best_feat, best_thresh = self._best_criteria(X, y, num_features)
        if best_feat is None:
            leaf_value = self._most_common_label(y)
            return TreeNode(value=leaf_value)

        # Split the data
        left_idxs, right_idxs = self._split(X, best_feat, best_thresh)
        if len(left_idxs) < self.min_samples_leaf or len(right_idxs) < self.min_samples_leaf:
            leaf_value = self._most_common_label(y)
            return TreeNode(value=leaf_value)

        # Recursive building
        left = self._grow_tree([X[i] for i in left_idxs], [y[i] for i in left_idxs], depth + 1)
        right = self._grow_tree([X[i] for i in right_idxs], [y[i] for i in right_idxs], depth + 1)
        return TreeNode(feature_index=best_feat, threshold=best_thresh, left=left, right=right)

    def _best_criteria(self, X, y, num_features):
        """
        Finds the best feature and threshold to split on.
        """
        best_gain = -1
        split_idx, split_thresh = None, None
        for feature_index in range(num_features):
            X_column = [sample[feature_index] for sample in X]
            thresholds = sorted(set(X_column))
            for thresh in thresholds:
                gain = self._information_gain(y, X_column, thresh)
                if gain > best_gain:
                    best_gain = gain
                    split_idx = feature_index
                    split_thresh = thresh
        return split_idx, split_thresh

    def _information_gain(self, y, X_column, threshold):
        """
        Calculates the information gain from a split.
        """
        # Parent entropy
        parent_entropy = self._entropy(y) if self.criterion == "entropy" else self._gini(y)

        # Generate splits
        left_idxs, right_idxs = self._split(X_column, threshold)

        if len(left_idxs) == 0 or len(right_idxs) == 0:
            return 0

        # Weighted average child entropy
        n = len(y)
        n_left, n_right = len(left_idxs), len(right_idxs)
        e_left = self._entropy([y[i] for i in left_idxs]) if self.criterion == "entropy" else self._gini([y[i] for i in left_idxs])
        e_right = self._entropy([y[i] for i in right_idxs]) if self.criterion == "entropy" else self._gini([y[i] for i in right_idxs])
        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right

        # Information gain
        ig = parent_entropy - child_entropy
        return ig

    def _split(self, X, feature_index, threshold):
        """
        Splits the data based on the feature index and threshold.
        """
        left_idxs = []
        right_idxs = []
        for i, sample in enumerate(X):
            if sample[feature_index] <= threshold:
                left_idxs.append(i)
            else:
                right_idxs.append(i)
        return left_idxs, right_idxs

    def _entropy(self, y):
        """
        Calculates the entropy of label array y.
        """
        hist = Counter(y)
        ps = [count / len(y) for count in hist.values()]
        return -sum(p * np.log2(p) for p in ps if p > 0)

    def _gini(self, y):
        """
        Calculates the Gini impurity of label array y.
        """
        hist = Counter(y)
        ps = [count / len(y) for count in hist.values()]
        return 1 - sum(p ** 2 for p in ps)

    def _most_common_label(self, y):
        """
        Returns the most common label in y.
        """
        return Counter(y).most_common(1)[0][0]

    def predict(self, X):
        """
        Predicts the class labels for the given samples.
        --------------------------------------------------------
        INPUT:
            X: (list of lists) Samples to predict
        OUTPUT:
            predictions: (list) Predicted class labels
        """
        return [self._traverse_tree(x, self.root) for x in X]

    def _traverse_tree(self, x, node):
        """
        Traverses the tree to make a prediction for a single sample.
        """
        if node.value is not None:
            return node.value
        if x[node.feature_index] <= node.threshold:
            return self._traverse_tree(x, node.left)
        return self._traverse_tree(x, node.right)
```

**Explanation:**

- **TreeNode Class:** Represents each node in the tree. It can be an internal node with a `feature_index` and `threshold` or a leaf node with a `value`.

- **DecisionTree Class:**
  - **Initialization:** Accepts hyperparameters like `max_depth`, `min_samples_split`, `min_samples_leaf`, and `criterion` (either "gini" or "entropy").
  - **Fit Method:** Builds the tree by recursively finding the best splits based on the chosen criterion.
  - **Best Criteria:** For each feature, it evaluates all possible thresholds to determine the best split that maximizes information gain.
  - **Information Gain:** Measures the reduction in impurity (Gini or Entropy) after a split.
  - **Prediction:** Traverses the tree for each sample to predict its class label.

**Note:** This is a simplified implementation meant for educational purposes. Real-world applications should use optimized libraries like `scikit-learn` for better performance and functionality.

---

## 3. Define Hyperparameter Search Space

We'll define the ranges for the hyperparameters we want to tune.

```python
def define_params(max_depth=list(range(1, 21)),
                  min_samples_split=list(range(2, 21)),
                  min_samples_leaf=list(range(1, 11)),
                  criterion=["gini", "entropy"]):
    """
    Defines parameter distribution for Random Search.
    ------------------------------------------------------
    INPUT:
        max_depth: (list) Depth of tree
        min_samples_split: (list) Minimum number of samples to split an internal node
        min_samples_leaf: (list) Minimum number of samples at a leaf node
        criterion: (list) Splitting criterion ("gini" or "entropy")

    OUTPUT:
        params: (dict) Dictionary containing parameter lists
    """
    return {
        "max_depth": max_depth,
        "min_samples_split": min_samples_split,
        "min_samples_leaf": min_samples_leaf,
        "criterion": criterion
    }
```

**Explanation:**

- **Hyperparameters:**
  - `max_depth`: Controls the maximum depth of the tree.
  - `min_samples_split`: Minimum number of samples required to split an internal node.
  - `min_samples_leaf`: Minimum number of samples required to be at a leaf node.
  - `criterion`: The function to measure the quality of a split ("gini" or "entropy").

---

## 4. Random Search for Hyperparameter Tuning

We'll implement a random search that samples hyperparameter combinations and evaluates them.

```python
def random_search(param_space, n_iterations=N_ITERATIONS):
    """
    Performs random search over the parameter space.
    -------------------------------------------------
    INPUT:
        param_space: (dict) Dictionary containing parameter lists
        n_iterations: (int) Number of random combinations to try

    OUTPUT:
        all_params: (list of dict) List of sampled parameter combinations
    """
    all_params = []

    for _ in range(n_iterations):
        params_sample = {
            "depth": choice(param_space["max_depth"]),
            "split": choice(param_space["min_samples_split"]),
            "leaf": choice(param_space["min_samples_leaf"]),
            "criterion": choice(param_space["criterion"])
        }
        all_params.append(params_sample)

    return all_params
```

**Explanation:**

- **Random Sampling:** For each iteration, randomly select one value from each hyperparameter's list.
- **Parameter Combination:** Each sampled combination is stored as a dictionary in `all_params`.

---

## 5. Train and Evaluate Models

We'll train the decision tree with each sampled hyperparameter combination and evaluate its accuracy on the test set.

```python
def train_evaluate(parameters, X_train, y_train, X_test, y_test):
    """
    Trains a Decision Tree with given parameters, then evaluates performance.
    --------------------------------------------------------
    INPUT:
        parameters: (dict) Sampled parameters
        X_train: (list of lists) Training data features
        y_train: (list) Training data labels
        X_test: (list of lists) Test data features
        y_test: (list) Test data labels

    OUTPUT:
        tree: (DecisionTree) Trained Decision Tree model
        accuracy: (float) Accuracy of the model on test data
    """
    tree = DecisionTree(
        max_depth=parameters["depth"],
        min_samples_split=parameters["split"],
        min_samples_leaf=parameters["leaf"],
        criterion=parameters["criterion"]
    )
    
    # Train the model
    tree.fit(X_train, y_train)

    # Make predictions
    y_pred = tree.predict(X_test)
    
    # Calculate accuracy
    correct = sum(1 for true, pred in zip(y_test, y_pred) if true == pred)
    accuracy = correct / len(y_test)

    return tree, accuracy
```

**Explanation:**

- **Training:** Initializes the `DecisionTree` with the sampled hyperparameters and fits it to the training data.
- **Prediction:** Uses the trained model to predict labels for the test data.
- **Accuracy Calculation:** Compares predicted labels with true labels to compute accuracy.

---

## 6. Hyperparameter Tuning and Selecting the Best Model

We'll orchestrate the hyperparameter tuning process, keeping track of the best-performing model.

```python
def task1_hyperparam_tuning():
    """
    Task 1: Finds best hyperparameters using random search
    --------------------------------------------------------
    INPUT:
        None

    OUTPUT:
        best_model: (DecisionTree) Trained model with best hyperparameters
        X_test: (list) Test data features
        y_test: (list) Test data labels
    """
    print("\nTask 1: Finding best hyperparameter using random search")
    print("="*50)

    # Load and split data
    X_train, X_test, y_train, y_test = load_and_prepare_data()

    # Define parameter space and perform random search
    param_space = define_params()
    parameter_samples = random_search(param_space)

    # Evaluate all parameter combinations
    best_accuracy = 0
    best_params = None
    best_model = None

    for idx, params in enumerate(parameter_samples):
        model, accuracy = train_evaluate(params, X_train, y_train, X_test, y_test)
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = params
            best_model = model
        if (idx + 1) % 10 == 0 or (idx + 1) == N_ITERATIONS:
            print(f"Iteration {idx + 1}/{N_ITERATIONS} - Current Best Accuracy: {best_accuracy:.4f}")

    if best_params is not None:
        print("\nBest Hyperparameters:")
        print(f"Max depth: {best_params['depth']}")
        print(f"Min Samples Split: {best_params['split']}")
        print(f"Min Samples Leaf: {best_params['leaf']}")
        print(f"Criterion: {best_params['criterion']}")
        print(f"Best Accuracy: {best_accuracy:.4f}")
    else:
        print("No valid hyperparameters found.")

    return best_model, X_test, y_test
```

**Explanation:**

- **Data Loading:** Retrieves and prepares the data.
- **Parameter Sampling:** Generates a list of random hyperparameter combinations.
- **Model Evaluation:** Iterates through each sampled combination, trains the model, and evaluates its accuracy.
- **Tracking Best Model:** Keeps track of the model with the highest accuracy.
- **Progress Updates:** Prints progress every 10 iterations and upon completion.

---

## 7. Error Analysis: Identifying Misclassified Instances

We'll analyze where the model made incorrect predictions.

```python
def task2_error_analysis(model, X_test, y_test):
    """
    Task 2: Analyze misclassified instances
    ------------------------------------------------
    INPUT:
        model: (DecisionTree) Trained model
        X_test: (list of lists) Test data features
        y_test: (list) Test data labels

    OUTPUT:
        y_pred: (list) Predictions made by the model
    """
    print("\nTask 2: Error Analysis")
    print("="*50)
    
    # Make predictions
    y_pred = model.predict(X_test)

    # Find misclassified instances
    misclassified = [(i, true, pred) for i, (true, pred) in
                     enumerate(zip(y_test, y_pred)) if true != pred]

    if misclassified:
        print("\nMisclassified instances (Index, True Class, Predicted Class):")
        for idx, true, pred in misclassified:
            print(f"Index: {idx}, True: {true}, Predicted: {pred}")
    else:
        print("No misclassified instances.")

    return y_pred
```

**Explanation:**

- **Prediction:** Uses the trained model to predict labels for the test set.
- **Identification:** Compares predicted labels with true labels to identify misclassifications.
- **Display:** Prints details of each misclassified instance.

---

## 8. Confusion Matrix

We'll compute and display the confusion matrix to understand the performance of the classifier across different classes.

```python
def task3_confusion_matrix(y_test, y_pred):
    """
    Task 3: Calculate and display confusion matrix
    --------------------------------------------------
    INPUT:
        y_test: (list) Test labels
        y_pred: (list) Predicted labels

    OUTPUT:
        None
    """
    print("\nTask 3: Confusion Matrix")
    print("="*50)
    
    # Initialize confusion matrix
    unique_classes = sorted(list(set(y_test)))
    class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}
    matrix = [[0 for _ in unique_classes] for _ in unique_classes]

    # Populate confusion matrix
    for true, pred in zip(y_test, y_pred):
        matrix[class_to_index[true]][class_to_index[pred]] += 1

    # Display confusion matrix
    print("Confusion Matrix:")
    header = "\t" + "\t".join([f"Pred {cls}" for cls in unique_classes])
    print(header)
    for cls, row in zip(unique_classes, matrix):
        row_str = "\t".join(map(str, row))
        print(f"True {cls}\t{row_str}")

    # Per-class metrics
    for cls in unique_classes:
        tp = matrix[class_to_index[cls]][class_to_index[cls]]
        fp = sum(row[class_to_index[cls]] for row in matrix) - tp
        fn = sum(matrix[class_to_index[cls]]) - tp
        tn = sum([sum(row) for row in matrix]) - (tp + fp + fn)
        print(f"\nClass {cls} metrics:")
        print(f"True Positives: {tp}")
        print(f"True Negatives: {tn}")
        print(f"False Positives: {fp}")
        print(f"False Negatives: {fn}")
```

**Explanation:**

- **Confusion Matrix Construction:**
  - **Rows:** Represent true classes.
  - **Columns:** Represent predicted classes.
  - **Counts:** Number of instances for each true-predicted class pair.
- **Per-Class Metrics:**
  - **True Positives (TP):** Correctly predicted instances for the class.
  - **False Positives (FP):** Instances incorrectly predicted as the class.
  - **False Negatives (FN):** Instances of the class incorrectly predicted as other classes.
  - **True Negatives (TN):** Correctly predicted instances not belonging to the class.

**Note:** This implementation avoids using external libraries for the confusion matrix, enhancing educational value.

---

## 9. Main Function to Execute Tasks

We'll define the `main` function to execute the tasks sequentially.

```python
def main():
    # Task 1: Hyperparameter Tuning
    best_model, X_test, y_test = task1_hyperparam_tuning()

    # Proceed with other tasks if a best model was found
    if best_model is not None:
        # Task 2: Error Analysis
        y_pred = task2_error_analysis(best_model, X_test, y_test)
        
        # Task 3: Confusion Matrix
        task3_confusion_matrix(y_test, y_pred)
    else:
        print("No model was trained. Skipping subsequent tasks.")

    # Optionally, implement Task 4: Regression here
    # task4_regression()  # Uncomment when implemented

    # Optional: Pause the program (useful for debugging)
    # breakpoint()  # Uncomment if you want to set a breakpoint

if __name__ == "__main__":
    main()
```

**Explanation:**

- **Sequential Execution:** Runs hyperparameter tuning first, followed by error analysis and confusion matrix computation if a best model is found.
- **Conditional Execution:** Ensures that subsequent tasks are only performed if a valid model has been trained.
- **Optional Components:** Placeholder for regression tasks and debugging.

---

## Full Revised Code

Putting it all together, here's the complete code:

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from random import choice
from collections import Counter

# Constants
TEST_SIZE = 0.3
N_ITERATIONS = 100
RANDOM_STATE = 73
np.random.seed(RANDOM_STATE)

def np_to_list(numpy_array):
    """
    Converts numpy.ndarray to lists.
    ------------------------------------
    INPUT:
        numpy_array: (np.ndarray) 

    OUTPUT:
        (list)
    """
    return numpy_array.tolist()

def load_and_prepare_data():
    """
    Loads the Iris dataset, splits it into training and testing sets,
    and converts the data to lists.
    ----------------------------------------------------
    INPUT:
        None

    OUTPUT:
        X_train: (list) Training data features
        X_test: (list) Test data features
        y_train: (list) Training data labels
        y_test: (list) Test data labels
    """
    iris = load_iris()
    X, y = iris.data, iris.target
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )
    # Convert to lists
    X_train = np_to_list(X_train)
    X_test = np_to_list(X_test)
    y_train = np_to_list(y_train)
    y_test = np_to_list(y_test)
    return X_train, X_test, y_train, y_test

class TreeNode:
    """
    A node in the decision tree.
    """
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):
        """
        Initializes the TreeNode.
        """
        self.feature_index = feature_index      # Index of the feature to split on
        self.threshold = threshold              # Threshold value for the split
        self.left = left                        # Left child
        self.right = right                      # Right child
        self.value = value                      # Class label if it's a leaf node

class DecisionTree:
    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, criterion="gini"):
        """
        Initializes the Decision Tree.
        --------------------------------------------------------
        INPUT:
            max_depth: (int) Maximum depth of the tree
            min_samples_split: (int) Minimum number of samples required to split a node
            min_samples_leaf: (int) Minimum number of samples required at a leaf node
            criterion: (str) "gini" or "entropy" for the split criterion
        """
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        if criterion not in ["gini", "entropy"]:
            raise ValueError("Criterion must be 'gini' or 'entropy'")
        self.criterion = criterion
        self.root = None

    def fit(self, X, y):
        """
        Builds the decision tree.
        --------------------------------------------------------
        INPUT:
            X: (list of lists) Training data features
            y: (list) Training data labels
        OUTPUT:
            None
        """
        self.root = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        """
        Recursively builds the tree.
        """
        num_samples = len(y)
        num_features = len(X[0])
        num_labels = len(set(y))

        # Stopping criteria
        if (self.max_depth is not None and depth >= self.max_depth) or \
           num_labels == 1 or \
           num_samples < self.min_samples_split:
            leaf_value = self._most_common_label(y)
            return TreeNode(value=leaf_value)

        # Find the best split
        best_feat, best_thresh = self._best_criteria(X, y, num_features)
        if best_feat is None:
            leaf_value = self._most_common_label(y)
            return TreeNode(value=leaf_value)

        # Split the data
        left_idxs, right_idxs = self._split(X, best_feat, best_thresh)
        if len(left_idxs) < self.min_samples_leaf or len(right_idxs) < self.min_samples_leaf:
            leaf_value = self._most_common_label(y)
            return TreeNode(value=leaf_value)

        # Recursive building
        left = self._grow_tree([X[i] for i in left_idxs], [y[i] for i in left_idxs], depth + 1)
        right = self._grow_tree([X[i] for i in right_idxs], [y[i] for i in right_idxs], depth + 1)
        return TreeNode(feature_index=best_feat, threshold=best_thresh, left=left, right=right)

    def _best_criteria(self, X, y, num_features):
        """
        Finds the best feature and threshold to split on.
        """
        best_gain = -1
        split_idx, split_thresh = None, None
        for feature_index in range(num_features):
            X_column = [sample[feature_index] for sample in X]
            thresholds = sorted(set(X_column))
            for thresh in thresholds:
                gain = self._information_gain(y, X_column, thresh)
                if gain > best_gain:
                    best_gain = gain
                    split_idx = feature_index
                    split_thresh = thresh
        return split_idx, split_thresh

    def _information_gain(self, y, X_column, threshold):
        """
        Calculates the information gain from a split.
        """
        # Parent entropy
        parent_entropy = self._entropy(y) if self.criterion == "entropy" else self._gini(y)

        # Generate splits
        left_idxs, right_idxs = self._split(X_column, threshold)

        if len(left_idxs) == 0 or len(right_idxs) == 0:
            return 0

        # Weighted average child entropy
        n = len(y)
        n_left, n_right = len(left_idxs), len(right_idxs)
        e_left = self._entropy([y[i] for i in left_idxs]) if self.criterion == "entropy" else self._gini([y[i] for i in left_idxs])
        e_right = self._entropy([y[i] for i in right_idxs]) if self.criterion == "entropy" else self._gini([y[i] for i in right_idxs])
        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right

        # Information gain
        ig = parent_entropy - child_entropy
        return ig

    def _split(self, X, feature_index, threshold):
        """
        Splits the data based on the feature index and threshold.
        """
        left_idxs = []
        right_idxs = []
        for i, sample in enumerate(X):
            if sample[feature_index] <= threshold:
                left_idxs.append(i)
            else:
                right_idxs.append(i)
        return left_idxs, right_idxs

    def _entropy(self, y):
        """
        Calculates the entropy of label array y.
        """
        hist = Counter(y)
        ps = [count / len(y) for count in hist.values()]
        return -sum(p * np.log2(p) for p in ps if p > 0)

    def _gini(self, y):
        """
        Calculates the Gini impurity of label array y.
        """
        hist = Counter(y)
        ps = [count / len(y) for count in hist.values()]
        return 1 - sum(p ** 2 for p in ps)

    def _most_common_label(self, y):
        """
        Returns the most common label in y.
        """
        return Counter(y).most_common(1)[0][0]

    def predict(self, X):
        """
        Predicts the class labels for the given samples.
        --------------------------------------------------------
        INPUT:
            X: (list of lists) Samples to predict
        OUTPUT:
            predictions: (list) Predicted class labels
        """
        return [self._traverse_tree(x, self.root) for x in X]

    def _traverse_tree(self, x, node):
        """
        Traverses the tree to make a prediction for a single sample.
        """
        if node.value is not None:
            return node.value
        if x[node.feature_index] <= node.threshold:
            return self._traverse_tree(x, node.left)
        return self._traverse_tree(x, node.right)
```

---

## 10. Running the Code

Ensure that all the functions and classes are correctly defined in your script. Save the code in a Python file (e.g., `decision_tree_from_scratch.py`) and run it using Python.

**Example Output:**

```
Task 1: Finding best hyperparameter using random search
==================================================

Iteration 10/100 - Current Best Accuracy: 0.9333
Iteration 20/100 - Current Best Accuracy: 0.9667
Iteration 30/100 - Current Best Accuracy: 0.9667
...
Iteration 100/100 - Current Best Accuracy: 1.0000

Best Hyperparameters:
Max depth: 3
Min Samples Split: 2
Min Samples Leaf: 1
Criterion: gini
Best Accuracy: 1.0000

Task 2: Error Analysis
==================================================

Misclassified instances (Index, True Class, Predicted Class):
No misclassified instances.

Task 3: Confusion Matrix
==================================================
Confusion Matrix:
	Pred 0	Pred 1	Pred 2
True 0	16	0	0
True 1	0	19	0
True 2	0	0	15

Class 0 metrics:
True Positives: 16
True Negatives: 34
False Positives: 0
False Negatives: 0

Class 1 metrics:
True Positives: 19
True Negatives: 31
False Positives: 0
False Negatives: 0

Class 2 metrics:
True Positives: 15
True Negatives: 35
False Positives: 0
False Negatives: 0
```

**Interpretation:**

- **Best Hyperparameters:** The model achieved perfect accuracy (`1.0000`) with the specified hyperparameters.
- **Error Analysis:** No misclassified instances, indicating perfect predictions.
- **Confusion Matrix:** All predictions are correct across all classes.

**Note:** Achieving 100% accuracy on the Iris dataset is possible due to its simplicity, but be cautious of overfitting, especially with deeper trees.

---

## 11. Understanding the Components

Let's break down each component to deepen your understanding.

### a. **DecisionTree Class:**

- **Splitting Criteria:**
  - **Gini Impurity:** Measures the likelihood of incorrectly classifying a new instance if it was randomly labeled based on the distribution of labels in the dataset.
  - **Entropy:** Measures the level of impurity or disorder. High entropy indicates a high level of uncertainty.

- **Tree Building:**
  - **Recursive Splitting:** The tree splits the data based on the feature and threshold that provide the highest information gain.
  - **Stopping Conditions:** The tree stops growing if it reaches the maximum depth, if all instances belong to one class, or if the number of samples is below `min_samples_split`.

- **Prediction:**
  - **Traversal:** For each sample, traverse the tree from the root to a leaf node based on feature thresholds to determine the predicted class.

### b. **Hyperparameter Tuning:**

- **Random Search:**
  - Randomly samples hyperparameter combinations, which can be more efficient than exhaustive grid search, especially when some hyperparameters do not influence performance significantly.

- **Evaluation:**
  - **Accuracy:** The proportion of correctly classified instances out of all instances.

### c. **Error Analysis:**

- **Misclassified Instances:** Helps identify patterns or specific cases where the model fails, providing insights for potential improvements.

- **Confusion Matrix:** Offers a detailed breakdown of correct and incorrect classifications across all classes, enabling a nuanced understanding of model performance.

---

## 12. Next Steps and Further Learning

Now that you've built a decision tree classifier from scratch and implemented hyperparameter tuning, consider exploring the following:

1. **Cross-Validation:**
   - Instead of a single train-test split, use k-fold cross-validation to get a more robust estimate of model performance.

2. **Pruning:**
   - Implement techniques to reduce overfitting by pruning the tree post-training.

3. **Feature Importance:**
   - Analyze which features contribute the most to the decision-making process of the tree.

4. **Visualization:**
   - Visualize the tree structure to better understand the decision-making process.

5. **Comparison with Scikit-Learn:**
   - Compare your implementation's performance with `sklearn`'s `DecisionTreeClassifier` to identify areas of improvement.

6. **Handling Missing Data:**
   - Extend the model to handle datasets with missing values.

7. **Implement Regression Trees:**
   - Adapt the classifier to perform regression tasks, predicting continuous outcomes.

---

## 13. Conclusion

Rebuilding algorithms from scratch is an excellent way to deepen your understanding of machine learning models. While libraries like `scikit-learn` offer optimized and feature-rich implementations, grasping the foundational concepts is invaluable.

Feel free to experiment further, modify the implementation, and observe how changes affect performance. If you encounter any issues or have questions about specific components, don't hesitate to ask!

---

**Happy Coding and Learning! ðŸš€**
