from textwrap import dedent
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_iris, fetch_california_housing
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,
    mean_squared_error
)
import seaborn as sns
import pandas as pd

# Initial Constants
RANDOM_STATE = 73
TEST_SIZE = 0.2

# Set random seed
np.random.seed(RANDOM_STATE)

# Task 1: Load and split data
def load_split_data():
    """
    Loads both classification (iris) and regression (california) datasets and splits them.
    """
    # Classification data
    iris = load_iris()
    X_cls, y_cls = iris.data, iris.target

    # Split data
    X_cls_train, X_cls_test, y_cls_train, y_cls_test = train_test_split(
        X_cls, y_cls, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )

    # Regression data
    california = fetch_california_housing()
    X_reg, y_reg = california.data, california.target
    X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(
        X_reg, y_reg, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )

    return (
        X_cls_train, X_cls_test, y_cls_train, y_cls_test,
        X_reg_train, X_reg_test, y_reg_train, y_reg_test,
        iris, california
    )

# Define parameter distribution for Random Search
params = {
    "max_depth": range(1, 21),
    "min_samples_split": range(2, 21),
    "min_samples_leaf": range(1, 11),
    "criterion": ["gini", "entropy"]
}

# Random Search
def perform_random_search(X_train, y_train, params):
    """
    Implements RandomizedSearchCV to find optimal parameters for a Decision Tree Classifier.
    """
    # Initialize base classifier
    dt = DecisionTreeClassifier(random_state=RANDOM_STATE)
    
    # Random Search
    random_search = RandomizedSearchCV(
        estimator=dt,
        param_distributions=params,
        n_iter=100,
        cv=5,
        scoring="accuracy",
        n_jobs=-1,
        random_state=RANDOM_STATE,
        verbose=1
    )

    random_search.fit(X_train, y_train)

    return random_search

def analyze_best_model(X_train, y_train, params, iris):
    """
    Analyzes best model, outputting best parameters, cross-validation score, and model.
    """
    random_search = perform_random_search(X_train, y_train, params)
    print("\nModel Analysis Results")
    print("_" * 50)
    # Access parameters
    print(f"Best parameters: {random_search.best_params_}")
    # Best score
    print(f"Best cross-validation score: {random_search.best_score_:.4f}")

    # Get best model
    best_model = random_search.best_estimator_

    # Training score
    train_score = best_model.score(X_train, y_train)
    print(f"Training accuracy: {train_score:.4f}")

    # Feature importances
    feature_importance = pd.DataFrame({
        "feature": iris.feature_names,
        "importance": best_model.feature_importances_
    })
    print("\nFeature Importances:")
    print(feature_importance.sort_values('importance', ascending=False))

    return best_model

# Updated function to include all confusion matrix components
def analyze_classification_results(y_true, y_pred, class_names):
    """
    Performs classification analysis including complete Confusion Matrix metrics.
    ------------------------------------------------------
    INPUT:
        y_true: (np.ndarray) True labels
        y_pred: (np.ndarray) Predicted labels
        class_names: (list) Names of classes

    OUTPUT:
        tuple: (accuracy, precision, recall, f1)
    """
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average="weighted")
    recall = recall_score(y_true, y_pred, average="weighted")
    f1 = f1_score(y_true, y_pred, average="weighted")

    print("\nClassification Metrics")
    print("="*50)
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    print("\nConfusion Matrix Analysis:")
    print("-"*50)
    print("Detailed metrics for each class:")
    
    n_classes = len(class_names)
    for i in range(n_classes):
        # True Positives: Correct predictions for current class
        tp = cm[i, i]
        
        # False Positives: Other classes predicted as current class
        fp = np.sum(cm[:, i]) - tp
        
        # False Negatives: Current class predicted as other classes
        fn = np.sum(cm[i, :]) - tp
        
        # True Negatives: Correct predictions for all other classes
        tn = np.sum(cm) - (tp + fp + fn)
        
        print(f"\nClass {class_names[i]}:")
        print(f"True Positives (TP): {tp} - Correctly predicted {class_names[i]}")
        print(f"True Negatives (TN): {tn} - Correctly predicted other classes")
        print(f"False Positives (FP): {fp} - Incorrectly predicted as {class_names[i]}")
        print(f"False Negatives (FN): {fn} - {class_names[i]} incorrectly predicted as other classes")
        
        # Calculate class-specific metrics
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        print(f"Sensitivity (TPR): {sensitivity:.4f}")
        print(f"Specificity (TNR): {specificity:.4f}")

    # Visualize confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", xticklabels=class_names,
                yticklabels=class_names)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

    return accuracy, precision, recall, f1

# Updated error analysis function
def analyze_errors(best_model, X_test, y_test, iris):
    """
    Analyzes misclassified instances from the decision tree model.
    """
    # Make predictions
    y_pred = best_model.predict(X_test)
    
    # Find misclassified instances
    misclassified_mask = y_test != y_pred
    misclassified_indices = np.where(misclassified_mask)[0]
    
    print("\nError Analysis Results:")
    print("-" * 50)
    print(f"Number of misclassified instances: {len(misclassified_indices)}")
    print(f"Test set accuracy: {accuracy_score(y_test, y_pred):.4f}")
    
    if len(misclassified_indices) > 0:
        print("\nMisclassified Instances Details:")
        for idx in misclassified_indices:
            print(f"\nIndex: {idx}")
            print(f"True class: {iris.target_names[y_test[idx]]}")
            print(f"Predicted class: {iris.target_names[y_pred[idx]]}")
            print("Features:")
            for feature_name, value in zip(iris.feature_names, X_test[idx]):
                print(f"  {feature_name}: {value:.2f}")
            
            # Calculate decision path
            path = best_model.decision_path([X_test[idx]])
            print("\nDecision path features:")
            for node_id in path.indices:
                if node_id < len(best_model.tree_.feature) and \
                   best_model.tree_.feature[node_id] != -2:  # -2 indicates leaf
                    feature = iris.feature_names[best_model.tree_.feature[node_id]]
                    threshold = best_model.tree_.threshold[node_id]
                    print(f"  Split on {feature} at threshold {threshold:.2f}")
    
    return misclassified_indices

def train_evaluate_regression(X_train, X_test, y_train, y_test, feature_names):
    """
    Trains and evaluates Decision Tree Regression model.
    """
    # Initialize and train regression model
    dt_reg = DecisionTreeRegressor(random_state=RANDOM_STATE)
    dt_reg.fit(X_train, y_train)

    # Predict
    y_pred = dt_reg.predict(X_test)

    # MSE
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    print("\nRegression Metrics:")
    print("=" * 50)
    print(f"Mean Squared Error: {mse:.4f}")
    print(f"Root Mean Squared Error: {rmse:.4f}")

    # Feature Importance Analysis
    feature_importance = pd.DataFrame({
        "feature": feature_names,
        "importance": dt_reg.feature_importances_
    })
    print("\nFeature Importances (Regression):")
    print(feature_importance.sort_values("importance", ascending=False))
    
    # Visualize actual vs predicted values
    plt.figure(figsize=(8, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--")
    plt.xlabel("Actual Values")
    plt.ylabel("Predicted Values")
    plt.title("Actual vs Predicted Values (Regression)")
    plt.show()

    return (mse, rmse), dt_reg

def compare_models(cls_metrics, reg_metrics, cls_model, reg_model, iris,
                   california, X_reg_test, y_reg_test):
    """
    Compares performance of classification and regression models.
    """
    print("\nModel Comparison")
    print("="*50)

    # Classification metrics
    print("Classification Model Performance")
    print(f"- Accuracy: {cls_metrics[0]:.4f}")
    print(f"- Precision: {cls_metrics[1]:.4f}")
    print(f"- Recall: {cls_metrics[2]:.4f}")
    print(f"- F1 Score: {cls_metrics[3]:.4f}")

    # Debug prints to check array lengths
    print("\nDebug Information:")
    print(f"Length of iris.feature_names: {len(iris.feature_names)}")
    print(f"Length of cls_model.feature_importances_: {len(cls_model.feature_importances_)}")
    print(f"Length of california.feature_names: {len(california.feature_names)}")
    print(f"Length of reg_model.feature_importances_: {len(reg_model.feature_importances_)}")

    # Feature importance for classification with error handling
    try:
        cls_importance = pd.DataFrame({
            "feature": list(iris.feature_names),  # Convert to list explicitly
            "importance": list(cls_model.feature_importances_)
        })
        cls_importance = cls_importance.sort_values("importance", ascending=False)
        print("\nTop Classification Features:")
        print(cls_importance)
    except ValueError as e:
        print(f"\nError creating classification importance DataFrame: {e}")
        print("Feature names:", iris.feature_names)
        print("Feature importances:", cls_model.feature_importances_)

    # Regression metrics
    print("\nRegression Model Performance (California Housing)")
    print("="*50)
    print(f"- MSE: {reg_metrics[0]:.4f}")
    print(f"- RMSE: {reg_metrics[1]:.4f}")
    print(f"R^2 Score: {reg_model.score(X_reg_test, y_reg_test):.4f}")

    # Feature importance for regression with error handling
    try:
        reg_importance = pd.DataFrame({
            "feature": list(california.feature_names),  # Convert to list explicitly
            "importance": list(reg_model.feature_importances_)
        })
        reg_importance = reg_importance.sort_values("importance", ascending=False)
        print("\nTop Regression Features:")
        print(reg_importance)
    except ValueError as e:
        print(f"\nError creating regression importance DataFrame: {e}")
        print("Feature names:", california.feature_names)
        print("Feature importances:", reg_model.feature_importances_)

    print("\n1. Classification Model:")
    print("- Model's performance evaluated through accuracy and f1-score")
    print("- High precision/recall indicates good balance between false positives and negatives")

    print("\n2. Regression Model:")
    print("- RMSE provides error metric in same units as target variable")
    print("- Lower RMSE indicates better model performance")


def main():
    # Load and split data
    (X_cls_train, X_cls_test, y_cls_train, y_cls_test,
     X_reg_train, X_reg_test, y_reg_train, y_reg_test,
     iris, california) = load_split_data()
    
    # Task 1: Get best classification model
    best_cls_model = analyze_best_model(X_cls_train, y_cls_train, params, iris)
    
    # Task 2: Error Analysis
    misclassified_indices = analyze_errors(best_cls_model, X_cls_test, y_cls_test, iris)
    
    # Task 3: Confusion Matrix Analysis
    cls_pred = best_cls_model.predict(X_cls_test)
    cls_metrics = analyze_classification_results(y_cls_test, cls_pred, iris.target_names)
    
    # Task 4: Train and evaluate regression model
    reg_metrics, reg_model = train_evaluate_regression(
        X_reg_train, X_reg_test, y_reg_train, y_reg_test, california.feature_names
    )
    
    # Task 5: Compare models
    cls_metrics = (
        accuracy_score(y_cls_test, cls_pred),
        precision_score(y_cls_test, cls_pred, average='weighted'),
        recall_score(y_cls_test, cls_pred, average='weighted'),
        f1_score(y_cls_test, cls_pred, average='weighted')
    )
    
    compare_dt_models(cls_metrics, reg_metrics, best_cls_model, reg_model, iris, california)

if __name__ == "__main__":
    main()

